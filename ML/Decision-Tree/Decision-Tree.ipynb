{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树常用于分类问题, 也是最常用的数据挖掘算法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 邮件分类系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`K`近邻算法可以完成多分类问题, 但是最大的缺点是无法给出数据的内在含义, 决策树的主要优势就在于数据形式非常容易理解."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figures/tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树的构造\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**思路:**\n",
    "\n",
    "\n",
    "+ 从一堆原始数据中构造决策树, 首先我们讨论构造决策树的方法, 编写构造解决树的代码.\n",
    "+ 度量算法成功率的方法.\n",
    "+ 使用递归建立分类器, 并使用 `Matplotlib` 绘制决策树.\n",
    "+ 输入一些隐性数据, 使用决策树分类器进行数据分类."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树的一般流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 收集数据:可以使用任何方法\n",
    "+ 准备数据: 树构造算法只使用于标称型数据, 因此数值型数据必须离散化.\n",
    "+ 分析数据:可以使用任何方法, 构造树完成之后, 我们应该检查图形是否符合预期.\n",
    "+ 训练算法: 构造树的数据结构.\n",
    "+ 训练算法: 使用经验计算错误率.\n",
    "+ 使用算法: 此步骤可以适用于任何监督学习算法, 而使用决策树可以更好地理解数据的内在含义."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点`（node）`和有向边`（directed edge）`组成。\n",
    "\n",
    "结点有两种类型：\n",
    "\n",
    "内部结点`（internal node）`：表示一个特征或属性。\n",
    "叶结点`（leaf： node）`：表示一个类。\n",
    "用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在划分数据集之前之后发生的变化称为信息增益.关键在于如何计算信息增益.计算每一个特征值划分数据集获得的信息增益, 获得信息增益的最高特征就是最好的选择."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "熵定义为信息的期望, 如果待分类的事物可能划分多个类, 用符号 $x_i$ 的信息定义为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "l(x_i) = -log_2p(x_i)\n",
    "$$\n",
    "\n",
    "$p(x_i)$ 是选择该分类的概率."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息熵的计算公式:\n",
    "\n",
    "$$\n",
    "Ent(D) = -\\sum_{k=1}^{|y|}p_klog_sp_k\n",
    "$$\n",
    "\n",
    "$Ent(D)$ 的值越小, 则 $D$ 的纯度越高."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益的计算:\n",
    "\n",
    "\n",
    "$$\n",
    "Gain(D,a) = Ent(D) -\\sum_{v=1}^{V}\\frac{D^v}{D}Ent(D^v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择信息增益最大的于是它被选为划分的属性.对给个分之节点进行上述的计算.最終得到决策树."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算给定数据集的信息熵\n",
    "\n",
    "def clacShannonENt(dataSet):\n",
    "    numEntrie = len(dataSet)\n",
    "    labelCounts = {}\n",
    "    for featVec in dataSet:\n",
    "        currentLabel = featVec[-1]\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1\n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = np.float(labelCounts[key])/numEntrie\n",
    "        shannonEnt -= prob*np.log2(prob)\n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建数据集进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatDataSet():\n",
    "    dataSet = [[1,1,'yes'],\n",
    "              [1,1,'yes'],\n",
    "              [1,0,'no'],\n",
    "              [0,1,'no'],\n",
    "              [0,1,'no']]\n",
    "    label = ['no surfacing','flippers']\n",
    "    return dataSet,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97095059445466858"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet,label = creatDataSet()\n",
    "shannonEnt = clacShannonENt(dataSet)\n",
    "shannonEnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "熵越高,混合的数据越多.我们可以在数据集中添加更多的分类, 观察熵是如何变化的. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet[0][-1]='maybe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3709505944546687"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shannonEnt = clacShannonENt(dataSet)\n",
    "shannonEnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们学习如何划分数据集以及如何度量信息熵."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 `C4.5` 决策树算法, 不直接使用信息增益, 使用的是增益率.来选择最优划分属性,计算公式为\n",
    "\n",
    "$$\n",
    "Gain Ratio(D,a) = \\frac{Gain(D,a)}{IV(a)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 \n",
    "\n",
    "$$\n",
    "IV(a) = -\\sum_{v=1}^{V}log_2\\frac{D^v}{D}\n",
    "$$\n",
    "\n",
    "成为属性 a 的固有值, 属性 a 的可能取值数目越越多(V越大) 则,IV(a)　通常越大.　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增益率的准则对可取数目较少的属性有所偏好.先从候选划分属性中赵出信息增益高于平均水平的属性.再从中选出增益率最高的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基尼指数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CART` 决策树使用的基尼指数来选择划分属性,　数据集(D)的纯度可以使用基尼指数来度量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Gini(D) = \\sum_{k=1}^{|y|}\\sum_{k'\\neq k}p^kp_k' =1 - \\sum_{k=1}^{|y|}p_k^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 Gini(D) 反映从数据集 D 中随机算去两个样本, 其类别标记不一致的概率.\n",
    "\n",
    "Gini(D) 越小数据集纯度越高.\n",
    "\n",
    "$$\n",
    "Gini_index(D,a) = \\sum_{v=1}^{V}\\frac{|D^v|}{|D|}Gini(D^v)\n",
    "$$\n",
    "\n",
    "我们选择划分后最小的基尼指数的属性作为最优的分类标准."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照特定的特征划分数据集.\n",
    "\n",
    "def splitDataSet(dataSet,axis,value):\n",
    "    reDataSet = []\n",
    "    for featVer in dataSet:\n",
    "        if featVer[axis] == value:\n",
    "            reducedFeatVec = featVer[:axis]\n",
    "            reducedFeatVec.extend(featVer[axis+1:])\n",
    "            reDataSet.append(reducedFeatVec)\n",
    "    return reDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 'maybe'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 'maybe']\n",
      "[1, 1, 'yes']\n",
      "[1, 0, 'no']\n",
      "[0, 1, 'no']\n",
      "[0, 1, 'no']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 'maybe'], [1, 'yes'], [0, 'no']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSet(dataSet,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'no'], [1, 'no']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSet(dataSet,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们将遍历整个数据集, 循环计算信息熵, 找到最好的特征划分."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择最好的数据集划分方式\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    baseEntropy = clacShannonENt(dataSet)\n",
    "    bestInforGain = 0.0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)\n",
    "        newEntorpy = 0.0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet,i,value)\n",
    "            prop = len(subDataSet)/np.float(len(dataSet))\n",
    "            newEntorpy += prop*clacShannonENt(subDataSet)\n",
    "        infoGain = baseEntropy - newEntorpy\n",
    "        if infoGain > bestInforGain:\n",
    "            bestInforGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chooseBestFeatureToSplit(dataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 递归构建决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote] = 0\n",
    "        classCount[vote] +=1\n",
    "    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建树函数的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建树函数的代码\n",
    "\n",
    "def creatTree(dataSet,label):\n",
    "    classList = [example[i] for example in dataSet]\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    \n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    bestFeatLabel = label[bestFeat]\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    del(label[bestFeat])\n",
    "    \n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    \n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = label[:]\n",
    "        mytree[bestFeatLabel][value] = creatTree(splitDataSet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
